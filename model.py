import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class GatedCnn(nn.Module):
    """docstring for GatedCnn"""
    def __init__(self, n_channel_in=102, n_channel_out=128, kernel_size=2, stride=1):
        super(GatedCnn, self).__init__()
        self.conv1 = nn.Conv1d(n_channel_in, n_channel_out, kernel_size, stride=stride)
        self.conv2 = nn.Conv1d(n_channel_in, n_channel_out, kernel_size, stride=stride)
        self.sigmoid = nn.Sigmoid()

    def forward(self, X):
        gate = self.conv1(X) # X: batch size * 102 * sequence_len
        gate = self.sigmoid(gate) # gate: batch * 128 * extracted
        out = self.conv2(X)
        out = gate * out

        return out # batch * 128 * extracted

class MyModel(nn.Module):
    def __init__(self, device, n_inputs=102, gated_cnn_outputs=128, gated_cnn_stride1=1, gated_cnn_stride2=1, gated_cnn_kernel1=2, gated_cnn_kernel2=3, lstm_layers=1, lstm_neurons=100, fc_outputs=64, dropout=0.5):
        super(MyModel, self).__init__()

        self.lstm_neurons = lstm_neurons
        self.n_inputs = n_inputs
        self.lstm_layers = lstm_layers
        self.device =device

        self.norm1 = nn.BatchNorm1d(self.n_inputs)
        self.gated_cnn1 = GatedCnn(self.n_inputs, gated_cnn_outputs, gated_cnn_kernel1, gated_cnn_stride1)
        self.gated_cnn2 = GatedCnn(self.n_inputs, gated_cnn_outputs, gated_cnn_kernel2, gated_cnn_stride2)
        self.norm2 = nn.BatchNorm1d(gated_cnn_outputs)
        self.dropout = nn.Dropout(0.2)
        self.lstm = nn.LSTM(gated_cnn_outputs, self.lstm_neurons, self.lstm_layers, bidirectional=True)

        self.left = nn.Sequential(
            nn.Linear(self.lstm_neurons*2, fc_outputs),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(fc_outputs, 1)
        )

    def init_hidden_and_cell(self, batch_size=1):
        return (torch.zeros(self.lstm_layers * 2, batch_size, self.lstm_neurons, device=self.device),
                torch.zeros(self.lstm_layers * 2, batch_size, self.lstm_neurons, device=self.device))

    def forward(self, X):
        # X: batch * input * seq_len
        batch = X.size(0)

        out = self.norm1(X)
        gate1 = self.gated_cnn1(out)
        gate2 = self.gated_cnn2(out)
        out = torch.cat([gate1, gate2], dim=2)
        out = self.norm2(out)
        hidden_and_cell = self.init_hidden_and_cell(out.size(0))
        out = out.permute(2, 0, 1)
        out = self.dropout(out)
        out, hidden = self.lstm(out, hidden_and_cell)
        out = out.permute(1, 2, 0)
        out = torch.max(out, 2)
        out = self.left(out[0]).view(batch)
        return out
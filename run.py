import numpy as np
import glob
import re
import pandas as pd
import torch
from model import MyModel
from train import Trainer
import time
import torch.optim as optim
import torch.nn as nn



def getDirName(type, suffix):
    if type in ['train', 'validation']:
        prefix = './tb/{}_'.format(type)
    elif type == 'model':
        prefix = './model/{}_'.format(type)
    else:
        raise AssertionError("Unexpected value of 'type'!", type)
    return prefix + suffix + '/'


torch.manual_seed(0)
np.random.seed(29)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

learning_rate = 0.001
t = str(int(time.time()))
logDirTrain = getDirName('train', t)
logDirVal = getDirName('validation', t)
modelSavDir = getDirName('model', t)
n_inputs=102
gated_cnn_outputs=128
gated_cnn_stride1=1
gated_cnn_stride2=1
gated_cnn_kernel1=2
gated_cnn_kernel2=3
lstm_layers=1
lstm_neurons=100
fc_outputs=64
dropout=0.5
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

epoch = 1000


model = MyModel(device, n_inputs=n_inputs, gated_cnn_outputs=gated_cnn_outputs, gated_cnn_stride1=gated_cnn_stride1, gated_cnn_stride2=gated_cnn_stride2, gated_cnn_kernel1=gated_cnn_kernel1, gated_cnn_kernel2=gated_cnn_kernel2, lstm_layers=lstm_layers, lstm_neurons=lstm_neurons, fc_outputs=fc_outputs, dropout=dropout)


optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.BCEWithLogitsLoss()

trainer = Trainer(model,  optimizer, criterion, logDirTrain, logDirVal, modelSavDir, device)
trainer.setUpSaveConfiguration(n_inputs, gated_cnn_outputs, gated_cnn_stride1, gated_cnn_stride2, gated_cnn_kernel1, gated_cnn_kernel2, lstm_layers, lstm_neurons, fc_outputs, dropout)
trainer.initTrainValidationLoader(50)

trainer.trainModel(epoch)
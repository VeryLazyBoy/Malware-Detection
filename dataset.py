import os
import glob
import re

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from tqdm.auto import tqdm


class MalwareDataset(Dataset):

    def __init__(self, inputDict, labelFrame, sequence):
        self.inputs = inputDict
        self.labels = labelFrame
        self.sequence = sequence


    def transformId(self, idx):
        return self.sequence[idx]

    def __len__(self):
        return len(self.sequence)

    def __getitem__(self, idx):
        idx = self.transformId(idx)
        input = torch.from_numpy(self.inputs[idx])
        label = torch.tensor(self.labels.loc[idx]).type(torch.FloatTensor)
        return (input, label)


class MalwareDatasetTrainValidationSplit():

    def __init__(self):
        inputs = {}

        inputFiles = glob.glob('train/*.npy')
        pbar = tqdm(total=len(inputFiles), desc='Starting to load input data...')
        for np_name in inputFiles:
            id = int(re.search(r'train[/\\](.*).npy', np_name).group(1))
            inputs[id] = np.load(np_name)
            pbar.update(1)

        labels = pd.read_csv('train_kaggle.csv', index_col='Id')

        all = np.arange(labels.shape[0])
        np.random.shuffle(all)
        train = all[:labels.shape[0]*95//100]
        val = all[labels.shape[0]*95//100:]
        print(val)

        self.train = MalwareDataset(inputs, labels, train)
        self.val = MalwareDataset(inputs, labels, val)

    def get(self):
        return (self.train, self.val)


class PadCollate:
    """
    a variant of callate_fn that pads according to the longest sequence in
    a batch of sequences
    """

    def __init__(self, dim=0):
        """
        args:
            dim - the dimension to be padded (dimension of time in sequences)
        """
        self.dim = dim

    def padTensor(self, tor, maxLength, dim):
        """
        args:
            tor - tensor to pad
            maxLength - the size to pad to
            dim - dimension to pad

        return:
            a new tensor padded to 'maxLength' in dimension 'dim'
        """
        padSize = list(tor.shape)
        padSize[dim] = maxLength - tor.size(dim)
        return torch.cat([tor, torch.zeros(*padSize)], dim=dim)

    def padCollate(self, batch):
        """
        args:
            batch - list of (input, label)

        reutrn:
            inputs - a tensor of all examples in 'batch' after padding
            labels - a FloatTensor of all labels in batch
        """
        # find longest sequence
        maxLength = max([(lambda x:x[0].shape[self.dim])(b) for b in batch])
        # pad according to maxLength
        batch = [(lambda x, y: (self.padTensor(x, maxLength, self.dim), y))(b[0], b[1]) for b in batch]
        # stack all

        inputs = torch.stack([b[0] for b in batch], dim=0)
        labels = torch.FloatTensor([b[1] for b in batch])
        return inputs, labels

    def __call__(self, batch):
        return self.padCollate(batch)

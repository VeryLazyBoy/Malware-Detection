import numpy as np
import pandas as pd
import torch
from model import MyModel
from dataset import PadCollate as pc
from dataset import MalwareDatasetTrainValidationSplit as md
from sklearn.metrics import roc_auc_score
import torch.nn as nn
from tensorboardX import SummaryWriter
import os
from tqdm.auto import tqdm

class Trainer():

    def __init__(self, model, optimizer, criterion, logDirTrain, logDirVal, modelSaveDir, device):
        self.device = device

        self.model = model
        if device.type == 'cpu':
            self.model.cpu()
        else:
            self.model.cuda(device=self.device)
        self.optimizer = optimizer
        self.criterion = criterion
        self.trainWriter = SummaryWriter(logDirTrain) # tensorboard --logdir=tb
        self.logDirTrain = logDirTrain
        self.valWriter = SummaryWriter(logDirVal)
        self.logDirVal = logDirVal
        self.modelSaveDir = modelSaveDir

        self.sigmoid = nn.Sigmoid()

        # Dummy values first, set later
        self.numInput = 0
        self.numGatedCnnOutput = 0
        self.gatedCnnStride1 = 0
        self.gatedCnnStride2 = 0
        self.gatedCnnKernel1 = 0
        self.gatedCnnKernel2 = 0
        self.lstmHiddenSize = 0
        self.numFcOutput = 0
        self.dropout = 0
        self.lstmLayer = 0
        self.batchCounter = 0
        self.epochTrainCounter = 0

    def setUpSaveConfiguration(self, numInput, numGatedCnnOutput, gatedCnnStride1, gatedCnnStride2, gatedCnnKernel1, gatedCnnKernel2,  lstmLayer, lstmHiddenSize, numFcOutput, dropout):
        """
        This method must be called before `trainModel` to set the parameters used
        for model saving.
        """
        self.numInput = numInput
        self.numGatedCnnOutput = numGatedCnnOutput
        self.gatedCnnStride1 = gatedCnnStride1
        self.gatedCnnStride2 = gatedCnnStride2
        self.gatedCnnKernel1 = gatedCnnKernel1
        self.gatedCnnKernel2 = gatedCnnKernel2
        self.lstmHiddenSize = lstmHiddenSize
        self.numFcOutput = numFcOutput
        self.dropout = dropout
        self.lstmLayer = lstmLayer

    def initTrainValidationLoader(self, batchSize, numWorker=0):
        trainSet, valSet = md().get()
        self.trainLoader = torch.utils.data.DataLoader(trainSet,
                                                       batch_size=batchSize,
                                                       shuffle=False,
                                                       num_workers=numWorker,
                                                       collate_fn=pc())
        self.valLoader = torch.utils.data.DataLoader(valSet,
                                                     batch_size=batchSize,
                                                     shuffle=True,
                                                     num_workers=numWorker,
                                                     collate_fn=pc())


    def trainModel(self, maxEpoch, validationInterval=1, startingEpoch=0):
        """
        Args:
            maxEpoch              (int) : number of epochs to train the model
            validationInterval    (int) : intervals between every log of validation metrics.
            startingEpoch         (int) : index of first epoch, useful to training resumption.
        """
        self.batchCounter = 0
        self.epochTrainCounter = startingEpoch
        try:
            # Loops over the dataset multiple times
            for epoch in list(range(maxEpoch))[startingEpoch:]:
                self._evaluateTrainSet(epoch)
                self.epochTrainCounter += 1
                if epoch % validationInterval == 0:
                    self._evaluateValidationSet(epoch)
                    self.saveModel(self.modelSaveDir, 'model{}.pt'.format(epoch))
        except KeyboardInterrupt:
            if self.epochTrainCounter == epoch:
                pass
            else:
                print("Waiting for validation")
                self.epochTrainCounter -= 1
                if epoch % validationInterval == 0:
                    self._evaluateValidationSet(epoch)
                    self.saveModel(self.modelSaveDir, 'model{}.pt'.format(epoch))
        finally:
            self.trainWriter.close()
            self.valWriter.close()
            print('Training is done.')

    def _evaluateTrainSet(self, epoch):
        trainRunningLoss = 0.0
        allOutputs = []
        allLabels = []
        self.model.train()

        # TRAINING ROUND
        i = None
        pbar = tqdm(total=len(self.trainLoader), desc='Starting batch training...')
        for i, data in enumerate(self.trainLoader):
            # Zeroes the parameter gradients
            self.optimizer.zero_grad()


            # Gets the inputs
            inputs = data[0].permute(0,2,1).to(self.device)
            labels = data[1].view(inputs.size(0)).to(self.device)

            # forward + backward + optimize
            outputs = self.model(inputs)
            allOutputs.append(self.sigmoid(outputs))
            allLabels.append(labels)

            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            batchLoss = loss.detach().item()

            trainRunningLoss += batchLoss

            self.batchCounter += 1
            pbar.update(1)
        pbar.close()

        allOutputs = torch.cat(allOutputs)
        allLabels = torch.cat(allLabels)
        lossPerEpoch = trainRunningLoss / (i + 1)
        aucPerEpoch = roc_auc_score(allLabels.cpu().detach().numpy(), allOutputs.cpu().detach().numpy())

        trainLogFormat = 'Epoch:  %d | Loss: %.4f | Auc: %.4f |'
        print(trainLogFormat % (epoch, lossPerEpoch, aucPerEpoch))
        self.trainWriter.add_scalar('EpochLoss', lossPerEpoch, epoch)
        self.trainWriter.add_scalar('EpochAuc', aucPerEpoch, epoch)


    def _evaluateValidationSet(self, epoch):
        valRunningLoss = 0.0
        allOutputs = []
        allLabels = []
        self.model.eval()

        # Deactivates autograd engine to save memory and increase speed
        with torch.no_grad():
            i = None
            for i, data in enumerate(self.valLoader):
                # Reshape is needed due to data loader
                inputs = data[0].permute(0,2,1).to(self.device)
                labels = data[1].view(inputs.size(0)).to(self.device)

                outputs = self.model(inputs)
                allOutputs.append(self.sigmoid(outputs))
                allLabels.append(labels)
                loss = self.criterion(outputs, labels)
                valRunningLoss += loss.item()

            allOutputs = torch.cat(allOutputs)
            allLabels = torch.cat(allLabels)
            lossPerEpoch = valRunningLoss / (i + 1)
            aucPerEpoch = roc_auc_score(allLabels.cpu().detach().numpy(), allOutputs.cpu().detach().numpy())

        valLogFormat = 'Validation Epoch:  %d | Loss: %.4f | Auc: %.4f |'
        print(valLogFormat % (epoch, lossPerEpoch, aucPerEpoch))
        self.valWriter.add_scalar('EpochLoss', lossPerEpoch, epoch)
        self.valWriter.add_scalar('EpochAuc', aucPerEpoch, epoch)

        self.model.train()

    def saveModel(self, fileDir, fileName):
        """
        Save the model for later testing.
        Args:
            fileDir  (str) : directory to save in
            fileName (str) : file name to save with
        """
        if not os.path.exists(fileDir):
            os.makedirs(fileDir)

        checkpoint = {'inputSize': self.numInput,
                      'gatedCnnOutputSize': self.numGatedCnnOutput,
                      'gatedCnnStride1': self.gatedCnnStride1,
                      'gatedCnnStride2': self.gatedCnnStride2,
                      'gatedCnnKernel1': self.gatedCnnKernel1,
                      'gatedCnnKernel2': self.gatedCnnKernel2,
                      'lstmHiddenSize': self.lstmHiddenSize,
                      'fcOutputSize': self.numFcOutput,
                      'dropout': self.dropout,
                      'lstmLayer': self.lstmLayer,
                      'stateDict': self.model.state_dict()}
        torch.save(checkpoint, fileDir + fileName)
